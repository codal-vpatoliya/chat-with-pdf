# from transformers import AutoModelForCausalLM, AutoTokenizer
# from app.config import QWEN_MODEL_NAME

# model_id = QWEN_MODEL_NAME

# # This will download and cache the model locally (e.g., ~/.cache/huggingface)
# tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
# model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)
